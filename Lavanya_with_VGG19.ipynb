{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "mount_file_id": "1d1uwvWpsbSQgCul8I5LDRh_ZtIEQsL2c",
      "authorship_tag": "ABX9TyO6PhCWKZ8DoWKh6u3MMVEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyasatpute/lavanyasatpute/blob/main/Lavanya_with_VGG19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "TkKojhRlhHRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i give the data from drive to colab i.e imges classification data with folder\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to your image data in Google Drive\n",
        "data_dir = '/content/drive/MyDrive/My Documents/final datset'\n",
        "\n",
        "# List the folders within the data directory\n",
        "folders = os.listdir(data_dir)\n",
        "print(data_dir)\n"
      ],
      "metadata": {
        "id": "BNVtFIi1iAe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to give it imges in that directory\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "for folder in folders:\n",
        "  folder_path = os.path.join(data_dir, folder)\n",
        "  if os.path.isdir(folder_path):\n",
        "    image_files = os.listdir(folder_path)\n",
        "    for image_file in image_files:\n",
        "      image_path = os.path.join(folder_path, image_file)\n",
        "      image = cv2.imread(image_path)\n",
        "      if image is not None:\n",
        "        # Preprocess the image if needed (e.g., resize, normalize)\n",
        "        # ...\n",
        "        image_data.append(image)\n",
        "        labels.append(folder)  # Use the folder name as the label\n"
      ],
      "metadata": {
        "id": "KduxMM0MjjS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[520]"
      ],
      "metadata": {
        "id": "IpLtcNbIlawO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code for spliting a data into validation and testing set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'data' is your dataframe\n",
        "train_data, temp_data = train_test_split(image_data, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "ho6teFuYhrEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label, temp_label = train_test_split(labels, test_size=0.2, random_state=42)\n",
        "val_label, test_label = train_test_split(temp_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "O0aULUJhlylI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "dataset_path = '/content/drive/MyDrive/My Documents/final datset'\n",
        "train_path = 'Dataset_splits/train'\n",
        "valid_path = 'Dataset_splits/valid'\n",
        "test_path = 'Dataset_splits/test'\n",
        "\n",
        "# Create directories for train, valid, and test splits\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(valid_path, exist_ok=True)\n",
        "os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "for cls in classes:\n",
        "    class_path = os.path.join(dataset_path, cls)\n",
        "    images = [os.path.join(class_path, f) for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "    # Split images into train, valid, and test sets\n",
        "    train_imgs, temp_imgs = train_test_split(images, test_size=0.30, random_state=42)  # 75% train, 15% valid + 15% test\n",
        "    valid_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.50, random_state=42)  # 50% of the remaining 30% = 15% valid, 15% test\n",
        "\n",
        "    # Create class directories in train, valid, and test splits\n",
        "    os.makedirs(os.path.join(train_path, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(valid_path, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_path, cls), exist_ok=True)\n",
        "\n",
        "    # Copy images to respective directories\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(img, os.path.join(train_path, cls))\n",
        "    for img in valid_imgs:\n",
        "        shutil.copy(img, os.path.join(valid_path, cls))\n",
        "    for img in test_imgs:\n",
        "        shutil.copy(img, os.path.join(test_path, cls))\n",
        "\n",
        "print(\"Dataset splitting complete!\")"
      ],
      "metadata": {
        "id": "QHqkEy9nvUha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQSyWxXbe1es"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define the path to your dataset\n",
        "train_data_dir = '/content/Dataset_splits/train'\n",
        "validation_data_dir = '/content/Dataset_splits/valid'\n",
        "weights_dir = '/content/model_weights/'  # Directory to save weights\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.makedirs(weights_dir)\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 512, 512\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Rescaling for validation set\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load the validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load the VGG19 model without the top classification layer\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up ModelCheckpoint to save the weights every 2 epochs\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=os.path.join(weights_dir, 'vgg19_weights_epoch_{epoch:02d}.weights.h5'),\n",
        "    save_weights_only=True,  # Only save the weights\n",
        "    save_freq='epoch'    # Save after every epoch\n",
        ")\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=800,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your test dataset\n",
        "test_data_dir = '/content/Dataset_splits/test'\n",
        "\n",
        "# Load the test data\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    test_generator,\n",
        "    steps=test_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "PF44bkY0mYRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}